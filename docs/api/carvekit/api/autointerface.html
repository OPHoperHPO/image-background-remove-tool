<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>carvekit.api.autointerface API documentation</title>
<meta name="description" content="Source url: https://github.com/OPHoperHPO/image-background-remove-tool
Author: Nikita Selin (OPHoperHPO)[https://github.com/OPHoperHPO].
License: â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>carvekit.api.autointerface</code></h1>
</header>
<section id="section-intro">
<p>Source url: <a href="https://github.com/OPHoperHPO/image-background-remove-tool">https://github.com/OPHoperHPO/image-background-remove-tool</a>
Author: Nikita Selin (OPHoperHPO)[<a href="https://github.com/OPHoperHPO].">https://github.com/OPHoperHPO].</a>
License: Apache License 2.0</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Source url: https://github.com/OPHoperHPO/image-background-remove-tool
Author: Nikita Selin (OPHoperHPO)[https://github.com/OPHoperHPO].
License: Apache License 2.0
&#34;&#34;&#34;
from collections import Counter
from pathlib import Path

from PIL import Image
from typing import Union, List, Dict

from carvekit.api.interface import Interface
from carvekit.ml.wrap.basnet import BASNET
from carvekit.ml.wrap.cascadepsp import CascadePSP
from carvekit.ml.wrap.deeplab_v3 import DeepLabV3
from carvekit.ml.wrap.fba_matting import FBAMatting
from carvekit.ml.wrap.scene_classifier import SceneClassifier
from carvekit.ml.wrap.tracer_b7 import TracerUniversalB7
from carvekit.ml.wrap.u2net import U2NET
from carvekit.ml.wrap.yolov4 import SimplifiedYoloV4
from carvekit.pipelines.postprocessing import CasMattingMethod
from carvekit.trimap.generator import TrimapGenerator

__all__ = [&#34;AutoInterface&#34;]

from carvekit.utils.image_utils import load_image

from carvekit.utils.pool_utils import thread_pool_processing


class AutoInterface(Interface):
    def __init__(
        self,
        scene_classifier: SceneClassifier,
        object_classifier: SimplifiedYoloV4,
        segmentation_batch_size: int = 3,
        refining_batch_size: int = 1,
        refining_image_size: int = 900,
        postprocessing_batch_size: int = 1,
        postprocessing_image_size: int = 2048,
        segmentation_device: str = &#34;cpu&#34;,
        postprocessing_device: str = &#34;cpu&#34;,
        fp16=False,
    ):
        &#34;&#34;&#34;
        Args:
            scene_classifier: SceneClassifier instance
            object_classifier: YoloV4_COCO instance
        &#34;&#34;&#34;
        self.scene_classifier = scene_classifier
        self.object_classifier = object_classifier
        self.segmentation_batch_size = segmentation_batch_size
        self.refining_batch_size = refining_batch_size
        self.refining_image_size = refining_image_size
        self.postprocessing_batch_size = postprocessing_batch_size
        self.postprocessing_image_size = postprocessing_image_size
        self.segmentation_device = segmentation_device
        self.postprocessing_device = postprocessing_device
        self.fp16 = fp16
        super().__init__(
            seg_pipe=None, post_pipe=None, pre_pipe=None
        )  # just for compatibility with Interface class

    @staticmethod
    def select_params_for_net(net: Union[TracerUniversalB7, U2NET, DeepLabV3]):
        &#34;&#34;&#34;
        Selects the parameters for the network depending on the scene

        Args:
            net: network
        &#34;&#34;&#34;
        if net == TracerUniversalB7:
            return {&#34;prob_threshold&#34;: 231, &#34;kernel_size&#34;: 30, &#34;erosion_iters&#34;: 5}
        elif net == U2NET:
            return {&#34;prob_threshold&#34;: 231, &#34;kernel_size&#34;: 30, &#34;erosion_iters&#34;: 5}
        elif net == DeepLabV3:
            return {&#34;prob_threshold&#34;: 231, &#34;kernel_size&#34;: 40, &#34;erosion_iters&#34;: 20}
        elif net == BASNET:
            return {&#34;prob_threshold&#34;: 231, &#34;kernel_size&#34;: 30, &#34;erosion_iters&#34;: 5}
        else:
            raise ValueError(&#34;Unknown network type&#34;)

    def select_net(self, scene: str, images_info: List[dict]):
        # TODO: Update this function, when new networks will be added
        if scene == &#34;hard&#34;:
            for image_info in images_info:
                objects = image_info[&#34;objects&#34;]
                if len(objects) == 0:
                    image_info[
                        &#34;net&#34;
                    ] = TracerUniversalB7  # It seems that the image is empty, but we will try to process it
                    continue
                obj_counter: Dict = dict(Counter([obj for obj in objects]))
                # fill empty classes
                for _tag in self.object_classifier.db:
                    if _tag not in obj_counter:
                        obj_counter[_tag] = 0

                non_empty_classes = [obj for obj in obj_counter if obj_counter[obj] &gt; 0]

                if obj_counter[&#34;human&#34;] &gt; 0 and len(non_empty_classes) == 1:
                    # Human only case. Hard Scene? It may be a photo of a person in far/middle distance.
                    image_info[&#34;net&#34;] = TracerUniversalB7
                    # TODO: will use DeepLabV3+ for this image, it is more suitable for this case,
                    #  but needs checks for small bbox
                elif obj_counter[&#34;human&#34;] &gt; 0 and len(non_empty_classes) &gt; 1:
                    # Okay, we have a human without extra hairs and something else. Hard border
                    image_info[&#34;net&#34;] = TracerUniversalB7
                elif obj_counter[&#34;cars&#34;] &gt; 0:
                    # Cars case
                    image_info[&#34;net&#34;] = TracerUniversalB7
                elif obj_counter[&#34;animals&#34;] &gt; 0:
                    # Animals case
                    image_info[&#34;net&#34;] = U2NET  # animals should be always in soft scenes
                else:
                    # We have no idea what is in the image, so we will try to process it with universal model
                    image_info[&#34;net&#34;] = TracerUniversalB7

        elif scene == &#34;soft&#34;:
            for image_info in images_info:
                objects = image_info[&#34;objects&#34;]
                if len(objects) == 0:
                    image_info[
                        &#34;net&#34;
                    ] = TracerUniversalB7  # It seems that the image is empty, but we will try to process it
                    continue
                obj_counter: Dict = dict(Counter([obj for obj in objects]))
                # fill empty classes
                for _tag in self.object_classifier.db:
                    if _tag not in obj_counter:
                        obj_counter[_tag] = 0

                non_empty_classes = [obj for obj in obj_counter if obj_counter[obj] &gt; 0]

                if obj_counter[&#34;human&#34;] &gt; 0 and len(non_empty_classes) == 1:
                    # Human only case. It may be a portrait
                    image_info[&#34;net&#34;] = U2NET
                elif obj_counter[&#34;human&#34;] &gt; 0 and len(non_empty_classes) &gt; 1:
                    # Okay, we have a human with hairs and something else
                    image_info[&#34;net&#34;] = U2NET
                elif obj_counter[&#34;cars&#34;] &gt; 0:
                    # Cars case.
                    image_info[&#34;net&#34;] = TracerUniversalB7
                elif obj_counter[&#34;animals&#34;] &gt; 0:
                    # Animals case
                    image_info[&#34;net&#34;] = U2NET  # animals should be always in soft scenes
                else:
                    # We have no idea what is in the image, so we will try to process it with universal model
                    image_info[&#34;net&#34;] = TracerUniversalB7
        elif scene == &#34;digital&#34;:
            for image_info in images_info:  # TODO: not implemented yet
                image_info[
                    &#34;net&#34;
                ] = TracerUniversalB7  # It seems that the image is empty, but we will try to process it

    def __call__(self, images: List[Union[str, Path, Image.Image]]):
        &#34;&#34;&#34;
        Automatically detects the scene and selects the appropriate network for segmentation

        Args:
            interface: Interface instance
            images: list of images

        Returns:
            list of masks
        &#34;&#34;&#34;
        loaded_images = thread_pool_processing(load_image, images)

        scene_analysis = self.scene_classifier(loaded_images)
        images_objects = self.object_classifier(loaded_images)

        images_per_scene = {}
        for i, image in enumerate(loaded_images):
            scene_name = scene_analysis[i][0][0]
            if scene_name not in images_per_scene:
                images_per_scene[scene_name] = []
            images_per_scene[scene_name].append(
                {&#34;image&#34;: image, &#34;objects&#34;: images_objects[i]}
            )

        for scene_name, images_info in list(images_per_scene.items()):
            self.select_net(scene_name, images_info)

        # groups images by net
        for scene_name, images_info in list(images_per_scene.items()):
            groups = {}
            for image_info in images_info:
                net = image_info[&#34;net&#34;]
                if net not in groups:
                    groups[net] = []
                groups[net].append(image_info)
            for net, gimages_info in list(groups.items()):
                sc_images = [image_info[&#34;image&#34;] for image_info in gimages_info]
                masks = net(
                    device=self.segmentation_device,
                    batch_size=self.segmentation_batch_size,
                    fp16=self.fp16,
                )(sc_images)

                for i, image_info in enumerate(gimages_info):
                    image_info[&#34;mask&#34;] = masks[i]

        cascadepsp = CascadePSP(
            device=self.postprocessing_device,
            fp16=self.fp16,
            input_tensor_size=self.refining_image_size,
            batch_size=self.refining_batch_size,
        )

        fba = FBAMatting(
            device=self.postprocessing_device,
            batch_size=self.postprocessing_batch_size,
            input_tensor_size=self.postprocessing_image_size,
            fp16=self.fp16,
        )
        # groups images by net
        for scene_name, images_info in list(images_per_scene.items()):
            groups = {}
            for image_info in images_info:
                net = image_info[&#34;net&#34;]
                if net not in groups:
                    groups[net] = []
                groups[net].append(image_info)
            for net, gimages_info in list(groups.items()):
                sc_images = [image_info[&#34;image&#34;] for image_info in gimages_info]
                # noinspection PyArgumentList
                trimap_generator = TrimapGenerator(**self.select_params_for_net(net))
                matting_method = CasMattingMethod(
                    refining_module=cascadepsp,
                    matting_module=fba,
                    trimap_generator=trimap_generator,
                    device=self.postprocessing_device,
                )
                masks = [image_info[&#34;mask&#34;] for image_info in gimages_info]
                result = matting_method(sc_images, masks)

                for i, image_info in enumerate(gimages_info):
                    image_info[&#34;result&#34;] = result[i]

        # Reconstructing the original order of image
        result = []
        for image in loaded_images:
            for scene_name, images_info in list(images_per_scene.items()):
                for image_info in images_info:
                    if image_info[&#34;image&#34;] == image:
                        result.append(image_info[&#34;result&#34;])
                        break
        if len(result) != len(images):
            raise RuntimeError(
                &#34;Something went wrong with restoring original order. Please report this bug.&#34;
            )
        return result</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="carvekit.api.autointerface.AutoInterface"><code class="flex name class">
<span>class <span class="ident">AutoInterface</span></span>
<span>(</span><span>scene_classifier:Â <a title="carvekit.ml.wrap.scene_classifier.SceneClassifier" href="../ml/wrap/scene_classifier.html#carvekit.ml.wrap.scene_classifier.SceneClassifier">SceneClassifier</a>, object_classifier:Â <a title="carvekit.ml.wrap.yolov4.SimplifiedYoloV4" href="../ml/wrap/yolov4.html#carvekit.ml.wrap.yolov4.SimplifiedYoloV4">SimplifiedYoloV4</a>, segmentation_batch_size:Â intÂ =Â 3, refining_batch_size:Â intÂ =Â 1, refining_image_size:Â intÂ =Â 900, postprocessing_batch_size:Â intÂ =Â 1, postprocessing_image_size:Â intÂ =Â 2048, segmentation_device:Â strÂ =Â 'cpu', postprocessing_device:Â strÂ =Â 'cpu', fp16=False)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>scene_classifier</code></strong></dt>
<dd>SceneClassifier instance</dd>
<dt><strong><code>object_classifier</code></strong></dt>
<dd>YoloV4_COCO instance</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AutoInterface(Interface):
    def __init__(
        self,
        scene_classifier: SceneClassifier,
        object_classifier: SimplifiedYoloV4,
        segmentation_batch_size: int = 3,
        refining_batch_size: int = 1,
        refining_image_size: int = 900,
        postprocessing_batch_size: int = 1,
        postprocessing_image_size: int = 2048,
        segmentation_device: str = &#34;cpu&#34;,
        postprocessing_device: str = &#34;cpu&#34;,
        fp16=False,
    ):
        &#34;&#34;&#34;
        Args:
            scene_classifier: SceneClassifier instance
            object_classifier: YoloV4_COCO instance
        &#34;&#34;&#34;
        self.scene_classifier = scene_classifier
        self.object_classifier = object_classifier
        self.segmentation_batch_size = segmentation_batch_size
        self.refining_batch_size = refining_batch_size
        self.refining_image_size = refining_image_size
        self.postprocessing_batch_size = postprocessing_batch_size
        self.postprocessing_image_size = postprocessing_image_size
        self.segmentation_device = segmentation_device
        self.postprocessing_device = postprocessing_device
        self.fp16 = fp16
        super().__init__(
            seg_pipe=None, post_pipe=None, pre_pipe=None
        )  # just for compatibility with Interface class

    @staticmethod
    def select_params_for_net(net: Union[TracerUniversalB7, U2NET, DeepLabV3]):
        &#34;&#34;&#34;
        Selects the parameters for the network depending on the scene

        Args:
            net: network
        &#34;&#34;&#34;
        if net == TracerUniversalB7:
            return {&#34;prob_threshold&#34;: 231, &#34;kernel_size&#34;: 30, &#34;erosion_iters&#34;: 5}
        elif net == U2NET:
            return {&#34;prob_threshold&#34;: 231, &#34;kernel_size&#34;: 30, &#34;erosion_iters&#34;: 5}
        elif net == DeepLabV3:
            return {&#34;prob_threshold&#34;: 231, &#34;kernel_size&#34;: 40, &#34;erosion_iters&#34;: 20}
        elif net == BASNET:
            return {&#34;prob_threshold&#34;: 231, &#34;kernel_size&#34;: 30, &#34;erosion_iters&#34;: 5}
        else:
            raise ValueError(&#34;Unknown network type&#34;)

    def select_net(self, scene: str, images_info: List[dict]):
        # TODO: Update this function, when new networks will be added
        if scene == &#34;hard&#34;:
            for image_info in images_info:
                objects = image_info[&#34;objects&#34;]
                if len(objects) == 0:
                    image_info[
                        &#34;net&#34;
                    ] = TracerUniversalB7  # It seems that the image is empty, but we will try to process it
                    continue
                obj_counter: Dict = dict(Counter([obj for obj in objects]))
                # fill empty classes
                for _tag in self.object_classifier.db:
                    if _tag not in obj_counter:
                        obj_counter[_tag] = 0

                non_empty_classes = [obj for obj in obj_counter if obj_counter[obj] &gt; 0]

                if obj_counter[&#34;human&#34;] &gt; 0 and len(non_empty_classes) == 1:
                    # Human only case. Hard Scene? It may be a photo of a person in far/middle distance.
                    image_info[&#34;net&#34;] = TracerUniversalB7
                    # TODO: will use DeepLabV3+ for this image, it is more suitable for this case,
                    #  but needs checks for small bbox
                elif obj_counter[&#34;human&#34;] &gt; 0 and len(non_empty_classes) &gt; 1:
                    # Okay, we have a human without extra hairs and something else. Hard border
                    image_info[&#34;net&#34;] = TracerUniversalB7
                elif obj_counter[&#34;cars&#34;] &gt; 0:
                    # Cars case
                    image_info[&#34;net&#34;] = TracerUniversalB7
                elif obj_counter[&#34;animals&#34;] &gt; 0:
                    # Animals case
                    image_info[&#34;net&#34;] = U2NET  # animals should be always in soft scenes
                else:
                    # We have no idea what is in the image, so we will try to process it with universal model
                    image_info[&#34;net&#34;] = TracerUniversalB7

        elif scene == &#34;soft&#34;:
            for image_info in images_info:
                objects = image_info[&#34;objects&#34;]
                if len(objects) == 0:
                    image_info[
                        &#34;net&#34;
                    ] = TracerUniversalB7  # It seems that the image is empty, but we will try to process it
                    continue
                obj_counter: Dict = dict(Counter([obj for obj in objects]))
                # fill empty classes
                for _tag in self.object_classifier.db:
                    if _tag not in obj_counter:
                        obj_counter[_tag] = 0

                non_empty_classes = [obj for obj in obj_counter if obj_counter[obj] &gt; 0]

                if obj_counter[&#34;human&#34;] &gt; 0 and len(non_empty_classes) == 1:
                    # Human only case. It may be a portrait
                    image_info[&#34;net&#34;] = U2NET
                elif obj_counter[&#34;human&#34;] &gt; 0 and len(non_empty_classes) &gt; 1:
                    # Okay, we have a human with hairs and something else
                    image_info[&#34;net&#34;] = U2NET
                elif obj_counter[&#34;cars&#34;] &gt; 0:
                    # Cars case.
                    image_info[&#34;net&#34;] = TracerUniversalB7
                elif obj_counter[&#34;animals&#34;] &gt; 0:
                    # Animals case
                    image_info[&#34;net&#34;] = U2NET  # animals should be always in soft scenes
                else:
                    # We have no idea what is in the image, so we will try to process it with universal model
                    image_info[&#34;net&#34;] = TracerUniversalB7
        elif scene == &#34;digital&#34;:
            for image_info in images_info:  # TODO: not implemented yet
                image_info[
                    &#34;net&#34;
                ] = TracerUniversalB7  # It seems that the image is empty, but we will try to process it

    def __call__(self, images: List[Union[str, Path, Image.Image]]):
        &#34;&#34;&#34;
        Automatically detects the scene and selects the appropriate network for segmentation

        Args:
            interface: Interface instance
            images: list of images

        Returns:
            list of masks
        &#34;&#34;&#34;
        loaded_images = thread_pool_processing(load_image, images)

        scene_analysis = self.scene_classifier(loaded_images)
        images_objects = self.object_classifier(loaded_images)

        images_per_scene = {}
        for i, image in enumerate(loaded_images):
            scene_name = scene_analysis[i][0][0]
            if scene_name not in images_per_scene:
                images_per_scene[scene_name] = []
            images_per_scene[scene_name].append(
                {&#34;image&#34;: image, &#34;objects&#34;: images_objects[i]}
            )

        for scene_name, images_info in list(images_per_scene.items()):
            self.select_net(scene_name, images_info)

        # groups images by net
        for scene_name, images_info in list(images_per_scene.items()):
            groups = {}
            for image_info in images_info:
                net = image_info[&#34;net&#34;]
                if net not in groups:
                    groups[net] = []
                groups[net].append(image_info)
            for net, gimages_info in list(groups.items()):
                sc_images = [image_info[&#34;image&#34;] for image_info in gimages_info]
                masks = net(
                    device=self.segmentation_device,
                    batch_size=self.segmentation_batch_size,
                    fp16=self.fp16,
                )(sc_images)

                for i, image_info in enumerate(gimages_info):
                    image_info[&#34;mask&#34;] = masks[i]

        cascadepsp = CascadePSP(
            device=self.postprocessing_device,
            fp16=self.fp16,
            input_tensor_size=self.refining_image_size,
            batch_size=self.refining_batch_size,
        )

        fba = FBAMatting(
            device=self.postprocessing_device,
            batch_size=self.postprocessing_batch_size,
            input_tensor_size=self.postprocessing_image_size,
            fp16=self.fp16,
        )
        # groups images by net
        for scene_name, images_info in list(images_per_scene.items()):
            groups = {}
            for image_info in images_info:
                net = image_info[&#34;net&#34;]
                if net not in groups:
                    groups[net] = []
                groups[net].append(image_info)
            for net, gimages_info in list(groups.items()):
                sc_images = [image_info[&#34;image&#34;] for image_info in gimages_info]
                # noinspection PyArgumentList
                trimap_generator = TrimapGenerator(**self.select_params_for_net(net))
                matting_method = CasMattingMethod(
                    refining_module=cascadepsp,
                    matting_module=fba,
                    trimap_generator=trimap_generator,
                    device=self.postprocessing_device,
                )
                masks = [image_info[&#34;mask&#34;] for image_info in gimages_info]
                result = matting_method(sc_images, masks)

                for i, image_info in enumerate(gimages_info):
                    image_info[&#34;result&#34;] = result[i]

        # Reconstructing the original order of image
        result = []
        for image in loaded_images:
            for scene_name, images_info in list(images_per_scene.items()):
                for image_info in images_info:
                    if image_info[&#34;image&#34;] == image:
                        result.append(image_info[&#34;result&#34;])
                        break
        if len(result) != len(images):
            raise RuntimeError(
                &#34;Something went wrong with restoring original order. Please report this bug.&#34;
            )
        return result</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="carvekit.api.interface.Interface" href="interface.html#carvekit.api.interface.Interface">Interface</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="carvekit.api.autointerface.AutoInterface.select_params_for_net"><code class="name flex">
<span>def <span class="ident">select_params_for_net</span></span>(<span>net:Â Union[<a title="carvekit.ml.wrap.tracer_b7.TracerUniversalB7" href="../ml/wrap/tracer_b7.html#carvekit.ml.wrap.tracer_b7.TracerUniversalB7">TracerUniversalB7</a>,Â <a title="carvekit.ml.wrap.u2net.U2NET" href="../ml/wrap/u2net.html#carvekit.ml.wrap.u2net.U2NET">U2NET</a>,Â <a title="carvekit.ml.wrap.deeplab_v3.DeepLabV3" href="../ml/wrap/deeplab_v3.html#carvekit.ml.wrap.deeplab_v3.DeepLabV3">DeepLabV3</a>])</span>
</code></dt>
<dd>
<div class="desc"><p>Selects the parameters for the network depending on the scene</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>net</code></strong></dt>
<dd>network</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def select_params_for_net(net: Union[TracerUniversalB7, U2NET, DeepLabV3]):
    &#34;&#34;&#34;
    Selects the parameters for the network depending on the scene

    Args:
        net: network
    &#34;&#34;&#34;
    if net == TracerUniversalB7:
        return {&#34;prob_threshold&#34;: 231, &#34;kernel_size&#34;: 30, &#34;erosion_iters&#34;: 5}
    elif net == U2NET:
        return {&#34;prob_threshold&#34;: 231, &#34;kernel_size&#34;: 30, &#34;erosion_iters&#34;: 5}
    elif net == DeepLabV3:
        return {&#34;prob_threshold&#34;: 231, &#34;kernel_size&#34;: 40, &#34;erosion_iters&#34;: 20}
    elif net == BASNET:
        return {&#34;prob_threshold&#34;: 231, &#34;kernel_size&#34;: 30, &#34;erosion_iters&#34;: 5}
    else:
        raise ValueError(&#34;Unknown network type&#34;)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="carvekit.api.autointerface.AutoInterface.select_net"><code class="name flex">
<span>def <span class="ident">select_net</span></span>(<span>self, scene:Â str, images_info:Â List[dict])</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def select_net(self, scene: str, images_info: List[dict]):
    # TODO: Update this function, when new networks will be added
    if scene == &#34;hard&#34;:
        for image_info in images_info:
            objects = image_info[&#34;objects&#34;]
            if len(objects) == 0:
                image_info[
                    &#34;net&#34;
                ] = TracerUniversalB7  # It seems that the image is empty, but we will try to process it
                continue
            obj_counter: Dict = dict(Counter([obj for obj in objects]))
            # fill empty classes
            for _tag in self.object_classifier.db:
                if _tag not in obj_counter:
                    obj_counter[_tag] = 0

            non_empty_classes = [obj for obj in obj_counter if obj_counter[obj] &gt; 0]

            if obj_counter[&#34;human&#34;] &gt; 0 and len(non_empty_classes) == 1:
                # Human only case. Hard Scene? It may be a photo of a person in far/middle distance.
                image_info[&#34;net&#34;] = TracerUniversalB7
                # TODO: will use DeepLabV3+ for this image, it is more suitable for this case,
                #  but needs checks for small bbox
            elif obj_counter[&#34;human&#34;] &gt; 0 and len(non_empty_classes) &gt; 1:
                # Okay, we have a human without extra hairs and something else. Hard border
                image_info[&#34;net&#34;] = TracerUniversalB7
            elif obj_counter[&#34;cars&#34;] &gt; 0:
                # Cars case
                image_info[&#34;net&#34;] = TracerUniversalB7
            elif obj_counter[&#34;animals&#34;] &gt; 0:
                # Animals case
                image_info[&#34;net&#34;] = U2NET  # animals should be always in soft scenes
            else:
                # We have no idea what is in the image, so we will try to process it with universal model
                image_info[&#34;net&#34;] = TracerUniversalB7

    elif scene == &#34;soft&#34;:
        for image_info in images_info:
            objects = image_info[&#34;objects&#34;]
            if len(objects) == 0:
                image_info[
                    &#34;net&#34;
                ] = TracerUniversalB7  # It seems that the image is empty, but we will try to process it
                continue
            obj_counter: Dict = dict(Counter([obj for obj in objects]))
            # fill empty classes
            for _tag in self.object_classifier.db:
                if _tag not in obj_counter:
                    obj_counter[_tag] = 0

            non_empty_classes = [obj for obj in obj_counter if obj_counter[obj] &gt; 0]

            if obj_counter[&#34;human&#34;] &gt; 0 and len(non_empty_classes) == 1:
                # Human only case. It may be a portrait
                image_info[&#34;net&#34;] = U2NET
            elif obj_counter[&#34;human&#34;] &gt; 0 and len(non_empty_classes) &gt; 1:
                # Okay, we have a human with hairs and something else
                image_info[&#34;net&#34;] = U2NET
            elif obj_counter[&#34;cars&#34;] &gt; 0:
                # Cars case.
                image_info[&#34;net&#34;] = TracerUniversalB7
            elif obj_counter[&#34;animals&#34;] &gt; 0:
                # Animals case
                image_info[&#34;net&#34;] = U2NET  # animals should be always in soft scenes
            else:
                # We have no idea what is in the image, so we will try to process it with universal model
                image_info[&#34;net&#34;] = TracerUniversalB7
    elif scene == &#34;digital&#34;:
        for image_info in images_info:  # TODO: not implemented yet
            image_info[
                &#34;net&#34;
            ] = TracerUniversalB7  # It seems that the image is empty, but we will try to process it</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="carvekit.api" href="index.html">carvekit.api</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="carvekit.api.autointerface.AutoInterface" href="#carvekit.api.autointerface.AutoInterface">AutoInterface</a></code></h4>
<ul class="">
<li><code><a title="carvekit.api.autointerface.AutoInterface.select_net" href="#carvekit.api.autointerface.AutoInterface.select_net">select_net</a></code></li>
<li><code><a title="carvekit.api.autointerface.AutoInterface.select_params_for_net" href="#carvekit.api.autointerface.AutoInterface.select_params_for_net">select_params_for_net</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>